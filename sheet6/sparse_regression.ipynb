{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import pylab as plt\n",
    "from collections import OrderedDict\n",
    "from numpy.linalg import lstsq, norm\n",
    "from sklearn import cross_validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) automatic feature selection for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) orthogonal matching pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def omp_regression(X, y, t):\n",
    "    \"\"\"\n",
    "    takes an NxD feature matrix X and an an Nx1 outcome y. \n",
    "    selects t features that maximally reduce the residuals.\n",
    "    X needs to be standardized.\n",
    "    \"\"\"\n",
    "    \n",
    "    # dimensions\n",
    "    N = X.shape[0]\n",
    "    D = X.shape[1]\n",
    "    \n",
    "    #initialization\n",
    "    A = [] #set of active columns in the order as selected\n",
    "    B = [i for i in range(D)] # set of inactive columns\n",
    "    r = np.array(y) #residual\n",
    "    current_X = np.empty((N, 1))\n",
    "    current_beta = None\n",
    "    \n",
    "    #iteratively find feature that is maximally correlated to the current residual\n",
    "    for i in range(t):\n",
    "        \n",
    "        #find the feature. j is the index in B!\n",
    "        col_reductions = [np.dot(X[:,k], y) for k in B]\n",
    "        j = np.argmax(np.absolute(col_reductions))\n",
    "        \n",
    "        #move it from A to B\n",
    "        A.append(B[j])\n",
    "        del B[j]\n",
    "        \n",
    "        #updating current_X\n",
    "        col = X[:, j]\n",
    "        col = np.expand_dims(col, 0)\n",
    "        if i == 0:\n",
    "            current_X = col.T\n",
    "        else:\n",
    "            current_X = np.concatenate((current_X, col.T), 1)\n",
    "            \n",
    "        #solving current least squares problem\n",
    "        lstsq_results = lstsq(current_X, y)\n",
    "        current_beta = lstsq_results[0]\n",
    "        \n",
    "        #updating residuals\n",
    "        r = y - np.dot(current_X, current_beta)\n",
    "    \n",
    "    #generating beta to return with t non-zero entries\n",
    "    beta = np.zeros(D)\n",
    "    for i in range(t):\n",
    "        dim = A[i]\n",
    "        beta[dim] = current_beta[i]\n",
    "        \n",
    "    return beta, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) LDA as sparse regression on digits 1 and 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_two_classes(raw_features, raw_labels, class1, class2, test_proportion):\n",
    "    '''function to prepare two classes for sparse regression. returning standardized data'''\n",
    "    \n",
    "    #standardize data\n",
    "    std_features = scale(raw_features)\n",
    "    \n",
    "    #splitting\n",
    "    x_train, x_test, y_train, y_test = train_test_split(std_features, \\\n",
    "                                        raw_labels, test_size=test_proportion)\n",
    "    \n",
    "    #class extraction\n",
    "    x_train_class1 = x_train[np.where(y_train == class1)]\n",
    "    x_train_class2 = x_train[np.where(y_train == class2)]\n",
    "    x_test = x_test[np.where((y_test==class1)|(y_test==class2))]\n",
    "    y_test = y_test[np.where((y_test==class1)|(y_test==class2))]\n",
    "    \n",
    "    return x_train_class1, x_train_class2, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasmoeller/anaconda/lib/python3.4/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(107, 1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n",
      "/Users/lucasmoeller/anaconda/lib/python3.4/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(104, -1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#loading data set\n",
    "digits = load_digits()\n",
    "\n",
    "data = digits['data']\n",
    "images = digits['images']\n",
    "target = digits['target']\n",
    "target_names = digits['target_names']\n",
    "\n",
    "#extracting ones and sevens and dividing into training and test set\n",
    "x_1, x_7, x_test, y_test = prepare_two_classes(data, target, 1, 7, 0.42)\n",
    "\n",
    "#concatenating training data to a single data matrix\n",
    "x_train = np.concatenate((x_1, x_7), 0)\n",
    "\n",
    "#generating corresponding output values, 1 for digit one and -1 for digit seven\n",
    "y_train = np.concatenate((np.full(len(x_1), 1), np.full(len(x_7), -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def two_class_error(guess, truth, class1=1, class2=7):\n",
    "    '''returns the total error rate and confusion matrix'''\n",
    "    \n",
    "    #absolute true and false classifications\n",
    "    false_class1 = len(guess[np.where((truth != class1) & (guess == class1))])\n",
    "    true_class1 = len(guess[np.where((truth == class1) & (guess == class1))])\n",
    "    total_class1 = len(truth[np.where(truth == class1)])\n",
    "    false_class2 = len(guess[np.where((truth != class2) & (guess == class2))])\n",
    "    true_class2 = len(guess[np.where((truth == class2) & (guess == class2))])\n",
    "    total_class2 = len(truth[np.where(truth == class2)])\n",
    "    false_total = len(guess[np.where(truth != guess)])\n",
    "    \n",
    "    #relative true and false classifications\n",
    "    error_rate = false_total/len(guess)\n",
    "    confusion_mat = np.array([[true_class1/total_class1, false_class2/total_class2], \\\n",
    "                                [false_class1/total_class1, true_class2/total_class2]])\n",
    "    \n",
    "    return error_rate, confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of features: 20\n",
      "selected features: [60, 19, 52, 61, 30, 53, 27, 6, 10, 14, 38, 20, 7, 3, 58, 29, 37, 26, 5, 63]\n",
      "total error rate: 0.23333333333333334\n",
      "confusion matrix:\n",
      " [[ 0.62666667  0.37333333]\n",
      " [ 0.09333333  0.90666667]]\n",
      "\n",
      "number of features: 18\n",
      "selected features: [60, 19, 52, 61, 30, 53, 27, 6, 10, 14, 38, 20, 7, 3, 58, 29, 37, 26]\n",
      "total error rate: 0.24\n",
      "confusion matrix:\n",
      " [[ 0.57333333  0.42666667]\n",
      " [ 0.05333333  0.94666667]]\n",
      "\n",
      "number of features: 15\n",
      "selected features: [60, 19, 52, 61, 30, 53, 27, 6, 10, 14, 38, 20, 7, 3, 58]\n",
      "total error rate: 0.013333333333333334\n",
      "confusion matrix:\n",
      " [[ 0.98666667  0.01333333]\n",
      " [ 0.01333333  0.98666667]]\n",
      "\n",
      "number of features: 12\n",
      "selected features: [60, 19, 52, 61, 30, 53, 27, 6, 10, 14, 38, 20]\n",
      "total error rate: 0.02666666666666667\n",
      "confusion matrix:\n",
      " [[ 0.97333333  0.02666667]\n",
      " [ 0.02666667  0.97333333]]\n",
      "\n",
      "number of features: 10\n",
      "selected features: [60, 19, 52, 61, 30, 53, 27, 6, 10, 14]\n",
      "total error rate: 0.03333333333333333\n",
      "confusion matrix:\n",
      " [[ 0.96        0.04      ]\n",
      " [ 0.02666667  0.97333333]]\n",
      "\n",
      "number of features: 5\n",
      "selected features: [60, 19, 52, 61, 30]\n",
      "total error rate: 0.02666666666666667\n",
      "confusion matrix:\n",
      " [[ 0.96        0.04      ]\n",
      " [ 0.01333333  0.98666667]]\n",
      "\n",
      "number of features: 3\n",
      "selected features: [60, 19, 52]\n",
      "total error rate: 0.013333333333333334\n",
      "confusion matrix:\n",
      " [[ 0.97333333  0.02666667]\n",
      " [ 0.          1.        ]]\n",
      "\n",
      "number of features: 2\n",
      "selected features: [60, 19]\n",
      "total error rate: 0.013333333333333334\n",
      "confusion matrix:\n",
      " [[ 0.97333333  0.02666667]\n",
      " [ 0.          1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucasmoeller/anaconda/lib/python3.4/site-packages/numpy/core/numeric.py:301: FutureWarning: in the future, full(150, 1) will return an array of dtype('int64')\n",
      "  format(shape, fill_value, array(fill_value).dtype), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def test_one_seven(x_train, y_train, x_test, y_test, feat_numbers):\n",
    "    '''evaluates the classification for different feature numbers'''\n",
    "    \n",
    "    #evaluation for every t\n",
    "    for t in feat_numbers:\n",
    "        \n",
    "        #regression / training\n",
    "        beta, features = omp_regression(x_train, y_train, t)\n",
    "        \n",
    "        #classifying test set\n",
    "        labels = np.dot(x_test, beta)\n",
    "        #translating results\n",
    "        estimates = np.full(len(labels), 1)\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i] < 0:\n",
    "                estimates[i] = 7\n",
    "                \n",
    "        #evaluation\n",
    "        err, conf = two_class_error(estimates, y_test)\n",
    "        \n",
    "        #printing results\n",
    "        print('\\nnumber of features:', t)\n",
    "        print('selected features:', features)\n",
    "        print('total error rate:', err)\n",
    "        print('confusion matrix:\\n', conf)\n",
    "        \n",
    "    return\n",
    "\n",
    "test_one_seven(x_train, y_train, x_test, y_test, [20, 18, 15, 12, 10, 5, 3, 2])        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is necessary to standardize the data. Otherwise the projection of features onto the residual vector in OMP is totally out of bounds.\n",
    "In exercise 2 we manually chose features 60 and 10 (by looking at mean and standard deviation of both classes).\n",
    "OMP chooses 60 and 19 which makes sense when looking back at the error bar plot we made in exercise two. These two features are the two whose distributions are the furthest appart from another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
